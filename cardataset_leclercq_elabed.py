# -*- coding: utf-8 -*-
"""Cardataset_Leclercq_Elabed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q_5hefF_g_G41wXEwNOtPVM2BdQ6jnW7
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

colonnes_names=['buying','maint','doors','persons','lug_boot','safety','class']
data =  pd.read_csv('/content/car.data',sep=",",names=colonnes_names)

data.head()

data.head()

for i in colonnes_names : 
  print(data[i].value_counts())

data['buying'] = data['buying'].replace(['vhigh'],4)
data['buying'] = data['buying'].replace(['high'],3)
data['buying'] = data['buying'].replace(['med'],2)
data['buying'] = data['buying'].replace(['low'],1)

data['maint'] = data['maint'].replace(['vhigh'],4)
data['maint'] = data['maint'].replace(['high'],3)
data['maint'] = data['maint'].replace(['med'],2)
data['maint'] = data['maint'].replace(['low'],1)

data['doors'] = data['doors'].replace(['5more'],5)
data['doors'] = data['doors'].replace(['4'],4)
data['doors'] = data['doors'].replace(['3'],3)
data['doors'] = data['doors'].replace(['2'],2)

data['persons'] = data['persons'].replace(['more'],5)
data['persons'] = data['persons'].replace(['4'],4)
data['persons'] = data['persons'].replace(['2'],2)

data['lug_boot'] = data['lug_boot'].replace(['big'],3)
data['lug_boot'] = data['lug_boot'].replace(['med'],2)
data['lug_boot'] = data['lug_boot'].replace(['small'],1)

data['safety'] = data['safety'].replace(['high'],3)
data['safety'] = data['safety'].replace(['med'],2)
data['safety'] = data['safety'].replace(['low'],1)

data.head()

from sklearn.model_selection import train_test_split

x = data.drop(['class'],axis = 1)
y = data['class']

xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size = 0.3, random_state = 0)
xtrain.shape, xtest.shape
xtrain.head()

xtrain.head()

xtest.head()

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree
import matplotlib.pyplot as plt


arbre1 = DecisionTreeClassifier()
arbre1.fit(xtrain,ytrain)

plt.figure(figsize=(50,12))  
tree.plot_tree(arbre1, fontsize=6, filled='true')
plt.show()
plt.savefig('insert', dpi=100)

ypred = arbre1.predict(xtest)
accuracy_score(ypred,ytest)

print(np.unique(ypred))

data['class'] = data['class'].replace(['unacc'],1)
data['class'] = data['class'].replace(['acc'],2)
data['class'] = data['class'].replace(['good'],3)
data['class'] = data['class'].replace(['vgood'],4)

x = data.drop(['class'],axis = 1)
y = data['class']

xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size = 0.3, random_state = 0)

from sklearn.model_selection import GridSearchCV
parameters = {"max_depth": range(1,120), "min_samples_leaf": range(1,10)}

GsCV = GridSearchCV(DecisionTreeClassifier(), parameters, scoring='neg_mean_squared_error', cv=5)
GsCV.fit(xtrain, ytrain)
ypred = GsCV.best_estimator_.predict(xtest)

print(GsCV.best_params_)

arbre2 = DecisionTreeClassifier(max_depth =31, min_samples_leaf =1 )
arbre2.fit(xtrain,ytrain)
ypred = arbre1.predict(xtest)
accuracy_score(ypred,ytest)

plt.figure(figsize=(50,12))  
tree.plot_tree(arbre2, fontsize=6, filled='true')
plt.show()
plt.savefig('insert', dpi=100)

# Paramètres
n_classes = 4
plot_colors = "bry" # blue-red-yellow
plot_step = 0.02
# On ne garde seulement les deux attributs
#X = data.values[:, 0:2]
X = data.drop(['doors', 'persons', 'lug_boot', 'safety', 'class'],axis = 1)
y = data['class']
# Apprentissage de l'arbre
clf = tree.DecisionTreeClassifier().fit(X, y)
# Affichage de la surface de décision
x_min, x_max = X.values[:, 0].min() - 1, X.values[:, 0].max() + 1
y_min, y_max = X.values[:, 1].min() - 1, X.values[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                     np.arange(y_min,y_max, plot_step))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
plt.axis("tight")
# Affichage des points d'apprentissage
for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X.values[idx, 0], X.values[idx, 1], c=color,
                label=data.values[i],cmap=plt.cm.Paired)
plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()

# Paramètres
n_classes = 4
plot_colors = "bry" # blue-red-yellow
plot_step = 0.02
# On ne garde seulement les deux attributs
#X = data.values[:, 0:2]
X = data.drop(['persons', 'maint', 'lug_boot', 'safety', 'class'],axis = 1)
y = data['class']
# Apprentissage de l'arbre
clf = tree.DecisionTreeClassifier().fit(X, y)
# Affichage de la surface de décision
x_min, x_max = X.values[:, 0].min() - 1, X.values[:, 0].max() + 1
y_min, y_max = X.values[:, 1].min() - 1, X.values[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                     np.arange(y_min,y_max, plot_step))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
plt.axis("tight")
# Affichage des points d'apprentissage
for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X.values[idx, 0], X.values[idx, 1], c=color,
                label=data.values[i],cmap=plt.cm.Paired)
plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()

